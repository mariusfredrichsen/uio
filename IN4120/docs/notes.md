# Notes

The following is a set of unstructured notes and opinions/commentary that may or may not useful.

When most people hear the word "search" they think of web search and services such as [Google](https://www.google.com/) or [Bing](https://www.bing.com/). But search also has a prominent place in the enterprise world, and search technologies come into play also when building feed- and recommendation products, advertising platforms, and more. As such, search covers much more than traditional web search, and is sometimes an invisible component to the application. Creatively overloading the notion of what a "document" or "query" is opens up for a lot of useful applications, and so-called "zero-term search" where the system automagially generates and executes a query on your behalf opens up for non-traditional scenarios.

Building a feature-rich, relevant, large-scale search engine is a multi-disciplinary endeavour, and involves drawing upon and putting together technologies from a variety of subfields in computer science such as information retrieval (IR), distributed systems, compression technologies, machine learning (ML) and artificial intelligence (AI), algorithms and data structures, natural language processing (NLP), human-computer interaction and UX design, and more. Many of these subfields partially overlap.

The current incarnation of this course largely has a focus on IR and so-called "full-text search", but occasionally deviates into neighbouring subfields. This both because of personal preferences, and because the field and world has clearly evolved in some key areas since the textbook was first published. Furthermore, it is limited what we can cram into a single semester, so some kind of topic selection needs to take place and this selection might be a bit biased towards my own personal propensities and not always aligned with the book (where the authors have also applied some topic selection criteria of their own.)

This course has morphed a bit over the years: There is a core based on IR basics, but every year I might choose to add or subtract a little based on what I happen to work on, or how the space generally evolves. The [textbook](https://nlp.stanford.edu/IR-book/information-retrieval-book.html) is a great introductory textbook, but is arguably a bit dated in places considering it was published in 2008. For example, listed in no particular order:

* The hardware landscape has evolved. For example, the [chapter on indexing](https://nlp.stanford.edu/IR-book/html/htmledition/index-construction-1.html) doesn't mention [SSDs](https://en.wikipedia.org/wiki/Solid-state_drive), probably because they were so expensive at the time and not in widespread use. With no spinning platters and no seek times, latency profiles change. The take-away still holds, though: Don't ignore the physics. But instead of focusing on absolute numbers that quickly become obsolete, it might be better to focus on the relative [costs of accessing data and moving data around](https://gist.github.com/jboner/2841832).

* The ML field has advanced, both when it comes to tooling, algorithms and applications. For example, although SVMs are still a great tool to have in the toolbox for many supervised classification problems, several algorithms not covered by the textbook (e.g., gradient-boosted decision trees or neural networks) have become goto weapons of choice. ML is a huge field in its own right, and this course barely even scrapes the surface. In practice, ML is key for ranking, content processing, and as part of query processing, and the role of ML has just become more important over time.

* Rankers have become almost exclusively ML-based. That doesn't mean that grasping concepts like, e.g., TF-IDF, PageRank, or cosine similarity are useless, far from it. But instead of them being components of a fixed ranking function, they typically become engineered ML features. Generally speaking, there's been a shift from sparse high-dimensional vector spaces (with one dimension per vocabulary term) and linear methods, to dense lower-dimensional vector spaces (more abstract "[embedding spaces](https://en.wikipedia.org/wiki/Latent_space)") and non-linear methods. Much of the same thinking around vector spaces in general still applies, though.

* Neural methods have really proven themselves. Dense embedding vector representations of text, images, graph nodes or whatever else you need to index have become mainstay, and dedicated index structures for efficient [approximate nearest neighbour matching](./slides/approximate-nearest-neighbours.pdf) in embedding spaces have appeared and become key in many applications. This is sometimes referred to as "semantic search" or "vector search".

* Generative AI has exploded. Search has become more of a "conversation", and [LLMs](https://en.wikipedia.org/wiki/Large_language_model) help summarize and stitch together fragments across many matching documents. (If you, through code and hands-on exploration, want to demystify how [transformer](./papers/attention-is-all-you-need.pdf)-based LLMs work, then [this](https://github.com/jaymody/picoGPT) might pique your interest.) Basic search technology still plays a fundamental role in retrieval-augmented generative AI applications such as [ChatGPT](https://openai.com/chatgpt): E.g., the queries being synthesized in the background by an orchestrating LLM are executed by an auxiliary search index, and the most relevant results from these searches are what get injected into the prompts as fresh and timely grounding data to be processed by a summarizing LLM. Search technology also plays a role when managing conversation context, e.g., when the length of the conversation exceeds the LLM's maximum prompt length.

* The compression field has advanced. The search industry might make use of other and newer compression algorithms than covered by the textbook, but many of the key ideas and core concepts are still the same, and so are the trade-offs. In particular, striking a good balance between compression ratio and decompression speed.

* Algorithms and data structures for strings are still important in industrial NLP and search. Data volume and the use of embedding spaces might have obviated much of the need for more "surface-level" string-trickery like stemming or synonym dictionaries, but fundamental data structures like, e.g., tries, automata, or Bloom filters and algorithms that rely on these are still crucial for many string-processing applications, even if they are not spotlighted in the textbook.
