# pylint: disable=missing-module-docstring
# pylint: disable=line-too-long
# pylint: disable=too-few-public-methods

from dataclasses import dataclass, field
from typing import Dict, List
import math
import random


class GradientBanditEngine:
    """
    Implements a gradient bandit algorithm, a stochastic gradient ascent algorithm used in reinforcement learning.
    See section 2.8 in the book by Sutton and Barto: http://www.incompleteideas.net/book/RLbook2018trimmed.pdf.

    Key concepts include:

    - **Actions (A)**: An action is something the system or a user can do. Sutton and Barto use A to denote the set
    of possible actions. The definition of an "action" depends on the specific problem at hand.

    - **Reward (R)**: After taking an action a ∈ A at time t, we receive feedback in the form of a numerical reward
    indicating how beneficial the action was. A higher reward implies a more successful action. Rewards can
    also be negative, which means the action was penalized.

    - **Preferences (H)**: For each possible action a ∈ A, we maintain a preference score that represents our
    inclination to take that action. Sutton and Barto use H to denote this vector of preferences. The higher the
    preference, the more likely the action is to be chosen. Using gradient ascent, we update H over time to
    maximize the expected cumulative reward.

    - **Policy (π)**: Preferences are transformed into a probability distribution over actions, known as the policy π.
    The next action is selected according to this policy, e.g., by sampling from it or by deterministically choosing
    the most probable action.

    Some example applications include:

    - **Personalized federated search**: Consider a scenario where a user’s query is sent to three search
    engines: X, Y, and Z. The system must merge their ranked results into a unified list without direct access
    to their ranking scores. Furthermore, different users may favor different search engines. Define the set of
    actions A as "select the top document from the queue produced by search engine S," where S ∈ {X, Y, Z}. To
    construct the final list, actions are repeatedly sampled from the policy π (which is derived from the user's
    preferences H). Each document in the final list is associated with an action a ∈ A, and we assign rewards based
    on user interactions with the final list: If the user clicks or dwells on a document for a long time, the reward
    is positive. But if the user skips or ignores the document, the reward is zero or negative. Preferences H are
    updated accordingly, refining π so that future search results better align with user behavior. A more advanced
    contextual bandit implementation could make preferences H dependent on the query q and other context signals (e.g.,
    time of day, or user location). For instance, H could be generated by a neural network that takes q and additional
    inputs, with gradient updates applied via backpropagation.

    - **Adaptive alert thresholds**: Consider a system that decides whether to send an alert based on the temperature
    of a machine. The goal is to allow the machine to run at maximum capacity while preventing overheating and potential
    failures. Instead of using a fixed temperature threshold, the system learns an adaptive threshold using reinforcement
    learning: The preference H represents the dynamic temperature threshold, the action is deciding whether to send an
    alert, and the reward depends on whether the alert was timely and useful. We give a positive reward if the alert
    successfully prevents a machine failure, and a negative reward if the alert is unnecessary or comes too late. Over
    time, the system refines H to minimize false alarms and missed warnings, ensuring optimal machine operation while
    avoiding downtime or unnecessary interventions.
    """

    @dataclass
    class Options:
        """
        Configuration options.

        The reward baseline used in the bandit's update algorithm can be tracked dynamically, if specified.
        This assumes that the bandit is presented with all feedback, also zero-reward items. If dynamic tracking
        is not specified, a static reward baseline of 0.0 is assumed.

        If the reward baseline is tracked dynamically, a discount factor is used. The tracked baseline is the
        exponential recency-weighted average reward. A value of 1.0 means that all the weight goes on the very last
        reward. A value smaller than 1.0 acts as a discounting factor, indicating how slow we decay older reward
        values when computing the average. See Section 2.5 in Sutton and Barto's book.

        The learning rate in gradient ascent is the step size α used when updating the underlying preferences.
        See Section 2.8 in Sutton and Barto's book.

        The "temperature", in allusion to statistical mechanics, is used when computing policy values from underlying
        preferences using soft-max normalization. For high temperatures (i.e., much higher than 1.0) all groups have
        nearly the same probability and the lower the temperature the more expected rewards affect the probability.
        For a low temperature (i.e., close to 0.0) the probability of the action with the highest expected rewards
        tends to 1. The temperature could be annealed, i.e., changed as learning progresses.
        """
        dynamic_baseline: bool = False  # Track the reward baseline dynamically?
        discount_factor: float = 0.1    # Discount factor to use, if we are tracking the reward baseline dynamically.
        learning_rate: float = 0.1      # The learning rate for gradient ascent.
        temperature: float = 1.0        # The "temperature" used when converting preference values to probabilities.

    @dataclass
    class State:
        """
        The current state of the gradient bandit.
        """
        baseline: float = 0.0                                        # The reward baseline B.
        preferences: Dict[str, float] = field(default_factory=dict)  # The preferences H.
        policy: Dict[str, float] = field(default_factory=dict)       # The policy π.

    def __init__(self, actions: List[str], options: Options):
        assert actions, "No actions specified."
        assert len(actions) == len(set(actions)), "Duplicate actions."
        assert options, "No options specified."
        assert 0.0 < options.discount_factor <= 1.0, "Discount factor must be in (0, 1]."
        assert options.learning_rate > 0.0, "Learning rate must be positive."
        assert options.temperature > 0.0, "Temperature must be positive."
        self._options = options
        self._state = self.State(0.0, {a: 0.0 for a in actions}, {a: 1.0 / len(actions) for a in actions})

    def _one(self, condition: bool) -> int:
        """
        Returns 1 if the condition holds, 0 otherwise.
        """
        return 1 if condition else 0

    def _softmax(self, preferences: Dict[str, float]) -> Dict[str, float]:
        """
        Transforms the set of preference values to a probability distribution using the
        safe softmax function. See https://en.wikipedia.org/wiki/Softmax_function for
        details. The resulting distribution is also known as a Gibbs or Boltzmann
        distribution.
        """
        largest = max(preferences.values())
        exponentiated = {a: math.exp((v - largest) / self._options.temperature) for a, v in preferences.items()}
        total = sum(exponentiated.values())
        return {a: v / total for a, v in exponentiated.items()}

    def update(self, action: str, reward: float) -> None:
        """
        Updates the preferences based on the received reward signal, and adjusts the policy accordingly.
        See section 2.8 in http://www.incompleteideas.net/book/RLbook2018trimmed.pdf for details.
        """
        assert action in self._state.preferences, "Unknown action."

        # Update the preferences.
        for a in self._state.preferences:
            gradient = (reward - self._state.baseline) * (self._one(a == action) - self._state.policy[a])
            self._state.preferences[a] += self._options.learning_rate * gradient

        # Update the reward baseline, if dynamic.
        if self._options.dynamic_baseline:
            self._state.baseline += self._options.discount_factor * (reward - self._state.baseline)

        # Update the policy, now that the preferences have been updated.
        self._state.policy = self._softmax(self._state.preferences)

    def state(self) -> State:
        """
        Returns the current internal state of the gradient bandit.
        """
        return self._state

    def histogram(self) -> str:
        """
        Returns a simple string representation of the current policy, visualized as a histogram.
        Facilitates testing and debugging.
        """
        width = max(len(a) for a in self._state.policy)
        largest = max(self._state.policy.values())
        lines = []
        for a in sorted(self._state.policy.keys()):
            value = self._state.policy[a]
            lines.append(f"{a.ljust(width)} ({value:.3f}): {'*' * int(50 * value / largest)}")
        return "\n".join(lines)

    def sample(self, k: int, subset: List[str] | None = None) -> List[str]:
        """
        Samples a given number of actions according to the current policy. Having a bias towards
        sampling more probable actions makes sense for exploitation, whereas leaving room for
        sampling less probable actions is beneficial for exploration.

        A subset of actions to sample from can be optionally supplied. If no subset is provided,
        all actions are sampled from.
        """
        assert k > 0, "Invalid sample size."
        assert subset is None or all(a in self._state.policy for a in subset), "Unknown action."
        assert subset is None or len(subset) > 0, "Empty subset."
        if subset is None:
            actions = list(self._state.policy.keys())
            probabilities = list(self._state.policy.values())
        else:
            actions = subset
            probabilities = [self._state.policy[a] for a in actions]
        return random.choices(actions, weights=probabilities, k=k)

    def greedy(self, epsilon: float = 0.0, subset: List[str] | None = None) -> str:
        """
        With a small probability ε, returns a uniformly sampled random action. With probability
        1 - ε, returns the action having the highest preference value. Ties are resolved arbitrarily.
        See Section 2.2 in http://www.incompleteideas.net/book/RLbook2018trimmed.pdf for details.

        A subset of actions to sample from can be optionally supplied. If no subset is provided,
        all actions are sampled from.
        """
        assert 0.0 <= epsilon <= 1.0, "Epsilon must be in [0, 1]."
        assert subset is None or all(a in self._state.policy for a in subset), "Unknown action."
        assert subset is None or len(subset) > 0, "Empty subset."
        if epsilon == 0.0 or random.uniform(0, 1) >= epsilon:
            return max((a for a in self._state.preferences if subset is None or a in subset), key=lambda x: self._state.preferences[x])
        if subset is None:
            return random.choice(list(self._state.policy.keys()))
        return random.choice(subset)
