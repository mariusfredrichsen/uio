{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 06: Linear and logistic regression\n",
    "### Introduction\n",
    "\n",
    "This week, we will get some first-hand experience with regression.\n",
    "We will implement gradient descent for linear regression. Then we will proceed to classification, first by using linear regression and then logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy\n",
    "We will first familiarize ourselves a little with NumPy. A function which we will use over again is `linspace(x1,x2,N)` which makes a vector of length $N$ splitting the interval $[x1,x2]$ into equally sized intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.linspace(-5,5,101)\n",
    "xx[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the major improvements from using NumPy is the possibilty of computing many values by applying a function to a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = -6*xx**3 + xx**2 -3*xx + 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y1`will contain the corresponding function values for each element `x`in `xx`. We may plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xx,y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset for linear regression\n",
    "We will start with a smooth curve and add some \"noise\". The underlying idea is that the smooth curve represents the function we are looking for, and that this is the best we can hope to learn. A solution which does better  on the training material than the smooth curve is probably overfit and will not generalize as well to new data as the smooth curve. We are using a normal distribution to generate noise. The numpy function `normal()` will generate a vector of `size` many random points around `loc` from a distribution with standard deviation `scale`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import normal\n",
    "t = y1 + normal(loc=0, scale=50, size=101)\n",
    "plt.scatter(xx, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training data now consists of pairs (xx[i], t[i]), where xx[i] is the datapoint and t[i] the target value. So far, both `xx` and `t` are vectors. Check their shapes, e.g., `xx.shape`. The goal is to make an implementation for linear regression which works with an arbitrary number of input features and not just one. We will therefore transform `xx` to a matrix of dimension $N\\times m$ where each row represents one datapoint, and m is the number of input variables (or features). Check the shape of `X` after the transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = xx.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Linear regression\n",
    "\n",
    "We will implement our own linear regression model. Our aim is to find an approximate function that fits the data generated above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are dealing with only one input variable, we start with a simple linear function, $f(x_1) = w_0 + w_1x_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: MSE\n",
    "\n",
    "We wonder if our $f$ fits the data well, and what parameters will give us the best approximation. We will estimate this using the Mean Squared Error:\n",
    "\n",
    "$\\frac{1}{N} \\sum_{j=1}^{N} (t_j - \\sum_{i=0}^{m} w_ix_{ji})^2$\n",
    "\n",
    "Write a function calculating MSE of our approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Testing the MSE\n",
    "To test our implementation, we can take the function $f(x_1)=0$ as a baseline and calculate the MSE for this $f$. Also calculate the Root Mean Square Error which provides a more natural measure for how good the fit is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Adding bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement linear regression with gradient descent and test it on the data. To make it simple, we will add a $x_0=1$ to all our datapoints, and consider $f(x_1) = w_0 + w_1x_1$ as $f(x_0, x_1) = w_0x_0+ w_1x_1$. Make a procedure that does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bias(X):\n",
    "    \"\"\"X is a Nxm matrix: N datapoints, m features\n",
    "    Return a Nx(m+1) matrix with added bias in position zero\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Gradient Descent\n",
    "We will implement the linear regression in a class as we did with the classifiers earlier. The `fit()` method will run the gradient descent step a number of times to train the classifier. The `predict()` method should take a matrix containing several data points and predict the outcome for all of them. Fill in the methods.\n",
    "\n",
    "Assume that the matrix of training data is not extended with bias features. Hence, make adding bias a part of your methods.\n",
    "\n",
    "After training there should be an attribute with learned coefficients (weights) which is applied by the `predict()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyLinReg():\n",
    "\n",
    "    def fit(self, X_train, t_train, eta = 0.1, epochs=10):\n",
    "        \"\"\"X_train is a Nxm matrix, N data points, m features\n",
    "        t_train is avector of length N,\n",
    "        the targets values for the training data\"\"\"\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"X is a Kxm matrix for some K>=1\n",
    "        predict the value for each point in X\"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Train and test the model\n",
    "Fit the model to the training data. Report the coefficients. Plot the line together with the observations. Calculate the RMSE. Is the result a better fit than the baseline constant function $f(x)=0$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset for classification\n",
    "We will use simple synthetic data similarly to week 04, but we will make the set a little bigger to get more reliable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X_train, y_train = make_blobs(n_samples=500, centers=[[0,0],[1,2]], \n",
    "                  n_features=2, random_state=2025)\n",
    "X_test, y_test = make_blobs(n_samples=500, centers=[[0,0],[1,2]], \n",
    "                  n_features=2, random_state=2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(X, y, marker='.'):\n",
    "    labels = set(y)\n",
    "    for lab in labels:\n",
    "        plt.plot(X[y == lab][:, 1], X[y == lab][:, 0],\n",
    "                 marker, label=f\"class {lab}\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression classifier\n",
    "This is also called Ridge Classifier in the literature when it is smoothed. We will consider the simple unsmoothed version here and return to smoothing and regularization later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Implementing the classifier\n",
    "Make a linear regression classifier. Make it as a Python class with methods for 'fit()' and 'predict()', similarly to the linear regression above and the *k*NN classifier from the week 04 exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: Experiment\n",
    "We will conduct repeated testing. We therefore need a development test set different from the final test set. Make such a set `X_dev`, `y_dev`, similarly to `X_test`, `y_test` using `random_state=2025`. Train the classfier on `X_train`, `y_train` and test for accuracy on `X_dev`, `y_dev`.\n",
    "\n",
    "You also need a procedure for accuracy here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8: The logistic function\n",
    "Implement the logistic function. Sometimes called just the sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x):\n",
    "    # fill in the rest\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9: Code for the classifier\n",
    "Write code for the logistic regression classifier. Compared to linear regression classifier you have to make adaptions to both `fit()` and `predict()` taking the logistic into consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10: Initial experiments\n",
    "Train the classfier on `X_train`, `y_train` and test for accuracy on `X_dev`, `y_dev`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11: Repeated experimentation\n",
    "Did you get better results than with the linear regression classifier? That does not necessary have to be the case for this data set. But, if your result is much inferior to the linear regression classifier, the reason might be the parameter settings. Experiment with the parameter values for the learning rate and the number of epochs to get an optimal result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of week 06"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
