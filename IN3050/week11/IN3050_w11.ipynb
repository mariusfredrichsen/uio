{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN3050/4050 - Week 11: Reinforcement Learning\n",
    "\n",
    "**Reinforcement Learning** (RL) is the last machine learning paradigm covered in this course. We will only take a cursory look at it. There is a lot of depth to this topic (just as for *supervised* and *unsupervised* learning), and we can of course not cover it in one single week. For those that are interested we can recomend the book \"Reinforcement Learning: An Introduction\" by Richard S. Sutton and Andrew G. Barto.\n",
    "\n",
    "In these exercises we will focus on understanding the main concepts, and implementing basic reinforcement learning scenarios and solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory\n",
    "\n",
    "Let us first redefine the main concepts underlying reinforcement learning:\n",
    "1. What is an agent?\n",
    "2. What is an environment?\n",
    "3. What is a state?\n",
    "4. What is an action?\n",
    "5. What is a policy?\n",
    "6. What is a reward?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: enter your answers here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "In this section we are going to run some simulations on a simple *GridWorld* environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We first import some support libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import constants as const\n",
    "import policies as plcs\n",
    "import worlds as wlds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module **constants** simply contains some handy aliases. Actions and objects in the environment are identified by integer numbers. This module allows you to refer to actions using the format *const.LEFT* instead of an integer number; similarly objects in the world can be referred to *const.WALL* instead of an integer number. Check out the source file if you want to see the association between aliases and their integer encoding.\n",
    "\n",
    "The module **policies** contains a set of pre-made policies that we will use in the exercise.\n",
    "\n",
    "The module **worlds** contains a set of pre-made gridworlds that we will use in the exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment: GridWorld\n",
    "Our environment is a simple GridWorld, that is a discrete flat rectangular environment made up $N\\times M$ squares, thus defining a grid. One square is identified as the *starting position*, and another square is the *ending position*. An agent start in the starting position, and its aim is to reach the ending position. A gridworld may be enriched with additional *walls* (that is, squares where the agent can not pass) and *traps* (that is, squares that kill the agent and terminate the episode).\n",
    "\n",
    "This gridworld model is **completely deterministic** (actions never fail) and **completely observed** (the agent perfectly knows the state of the world). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gridworld as gw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module **gridworld** contains the class *GridWorld* implementing the environment with which the agent interact. An object *GridWorld* offers the following interface:\n",
    "- **GridWorld(width,height, staring_position_x,staring_position_y, ending_position_x,ending_position_y)**: this is the constructor for a gridworld. It expects six parameters: the *width* and the *length* of the gridworld; the coordinates x and y of the starting position (*staring_position_x,staring_position_y*) and of the ending position (ending_position_x,ending_position_y). It returns an instance of a gridworld.\n",
    "- **add_wall(x,y)**: this is a function to add a wall to the gridworld in location *x,y*.\n",
    "- **add_trap(x,y)**: this is a function to add a trap to the gridworld in location *x,y*.\n",
    "- **step(action)**: this is a function that takes an *action* passed as a parameter. The gridworld processes the action and returns four values: the new *state* of the world (that is, the new position of the agent in the form of a Loc object with two attributes x and y); the *reward* obtained by the agent by performing the action; a boolean *termination* indicating whether the episode terminated; and, optionally, some debug messages. \n",
    "- **reset()**: this is a function to reset the environment to the starting condition; it returns the same values as step().\n",
    "- **print_basic()**: this is a function to produce a simple textual representation of the environment on the screen.\n",
    "- **print_unicode()**: this a slightly fancier printing function using Unicode characters (if support is available)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CAUTION**: A gridworld is indexed as a numpy matrix. Thus, if you build a $10 \\times 10$ matrix, the height and the width of the gridworld will be 10; however the square in the world will be indexed from 0 to 9, with (0,0) being the upper-left corner square, and (9,9) being the lower-right corner square. \n",
    "\n",
    "\n",
    "An example $3 \\times 3$ world:\n",
    "![title](grid.png)\n",
    "\n",
    "**Verification of consistency in *YOUR* hands!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing with the GridWorld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create now a $4 \\times 4$ gridworld with starting position in the upper-left corner and ending position in botttom-right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = gw.GridWorld(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us add a wall near the center in location (1,2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.add_wall(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the world using one of the supported print functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the basic rendering 0 is an open space, 1 is the starting position, 6 is a trap, 7 is a wall, 8 is the agent and 9 is the ending position.\n",
    "\n",
    "In the unicode rendering a white square is an open space, alpha is the starting position, a cross is a trap, a black square is a wall, the agent is a smile and omega is the ending position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resetting is a useful operation to restart the environment clean. It is a good practice to call it before starting any simulation to be certain that the environment is in the starting state. Call the reset function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the action of a step to the right, print the return values of the action, visualize the world again, and comment on the action and its consequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W.step(None))\n",
    "W.print_unicode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: enter your comment here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try now to take a step in the up direction, print the return values of the action, visualize, and comment on the action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: enter your comment here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lead the agent to the objective, print the return values of the action, visualize, and comment on the actions and the rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: enter your comment here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the world to the starting state, print the return values of the action, visualize, and comment on the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: enter your comment here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n",
    "\n",
    "One of the main problems in reinforcement learning is **policy evaluation**: *given a policy for acting in the environment, how good is the policy?*\n",
    "\n",
    "We will consider and answer this question first for simple *deterministic* policies (computing the simple overall return of a policy) and then for *stochastic* policies (using Monte Carlo to estimate the average overall return of a policy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the environment\n",
    "First load *world1()* from *wrlds* and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = wlds.world1()\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a deterministic policy\n",
    "We will first use the **deterministic policy** *plcs.policy1()*. This defines a policy that moves the agent always to the right until it reaches the border, and then heads down.\n",
    "\n",
    "Design a loop that makes the agent interact with the environment using this policy until termination. Visualize the output at each step to verify that the policy works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, termination, msgs = W.reset() \n",
    "\n",
    "while(None):\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit the previous loop to track the states traversed, the actions taken, and the rewards collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = []\n",
    "actions = []\n",
    "rewards = []\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the return of a deterministic policy\n",
    "One of the main quantities to evaluate the goodness of a policy is the *return*. While a *reward* quantifies the local/short-term goodness of a single action, the *return* quantifies the global/long-term goodness of a policy.\n",
    "\n",
    "Remember that we can compute the overall return of a policy $\\pi$ from time $t$ using a discount $\\gamma$ as:\n",
    "\n",
    "$$\n",
    "G_{\\pi}(t) = r(t) + \\sum_i \\gamma^i r(t+i),\n",
    "$$\n",
    "where $G_{\\pi}(t)$ is the return at time $t$ and r(t) is the reward at time $t$ under policy ${\\pi}$. \n",
    "\n",
    "Write code to compute the return of policy *plcs.policy1()* from the initial state at $t=0$ with a discount rate of $\\gamma = 0.9$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "\n",
    "G = None\n",
    "\n",
    "print('Return: {0}'.format(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run now the same code using the policy *plcs.policy2()*. This defines a deterministic policy that moves the agent first downwards, then upwards, and finally right and down to the objective. Plot the policy, compute its return and comment on the two policies you analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: enter your comment here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitation of the deterministic policy\n",
    "The deterministic policy *plcs.policy1()* is optimal for *wlds.world1()*. However deterministic policies have severe limitations.\n",
    "\n",
    "Load *world2* from *wlds*, plot the environment, and run 10 steps of *plcs.policy1()*. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = wlds.world2()\n",
    "W.print_unicode()\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: enter your comment here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a stochastic policy\n",
    "\n",
    "Having assessed the limitation of a deterministic policy, we now consider a **stochastic policy**. A stochastic policy adds some randomness in the selection of the action to perform. Taking random actions will allow the agent to *explore* more of the world and will provide a higher degree of *generalization*/*adaptability*.\n",
    "We use here *plcs.policy_epsilon1()*, a stochastic variant of *plcs.policy1()*, that behaves according to *plcs.policy1()*, except that 20% of the time it will take a random action.\n",
    "\n",
    "Run *plcs.policy1()* both on *world1* and *world2* and comment on the results. (You may want to run your simulation more than one time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = wlds.world1()\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = wlds.world2()\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: enter your comment here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the return of a stochastic policy\n",
    "As in the case of deterministic policies, we want to be able to quantify the goodness of a stochastic policy. However, the simple approach we adopted above will not immediately work in the case of a stochastic policy. Any two simulations using a stochastic policy will likely return different results, and computing returns on each of them will yield different values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Policy Evaluation (return)\n",
    "\n",
    "As discussed above, we can not evaluate the return of a stochastic policy deterministically from a single simulation. Indeed, two different simulations would yield different returns. An intuitive solution to this problem would be to compute the return of the stochastic policy *in expectation*.\n",
    "\n",
    "**Monte Carlo methods** constitute a wide family of techniques based on simulation and estimation of expected quantities. Imagine you are given a system whose dynamics are unknown or too hard to analyze analytically; suppose also that the system can be cheaply simulated; then we can try to answer questions about this system by repeatedly running simulations, collecting samples, and computing statistical quantities about the system.\n",
    "\n",
    "In our case, the dynamics of the environment (how we transition from one state to another given an action) are known and we could theoretically compute analytically the value of the policy. However we assume that this computation is too expensive and we rely on a Monte Carlo approximation. \n",
    "\n",
    "What we (and the agent) do is trying out a given policy many times; each repetition, from the starting state to termination, is called an *episode*; the list of actions, states and rewards collected during an episode is called a *trajectory*. Given a set of trajectories, we can compute the expected value of our policy.\n",
    "\n",
    "As you know, *Monte-Carlo* is just one approach to (tabular) policy evaluation in the case of stochastic policies. Alternatives are *dynamic programming* (where we assume the agent knows the dynamics of the environment) or *temporal differences* (where we assume that we do not have complete episodes).\n",
    "\n",
    "Load *wlds.world1()* and *plcs.policy_epsilon1()*, and run 50 episodes with this setting while collecting the trajectories of your simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 50\n",
    "episodes = []\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the data we have collected to compute the return in expectation:\n",
    "$$ G_{\\pi}(t) = \\mathbb{E} \\left[r(t) + \\sum_i \\gamma^i r(t+i)\\right]. $$\n",
    "\n",
    "This simply corresponds to the average return over the episodes:\n",
    "$$ \\hat{G}_{\\pi}(t) = \\frac{1}{N_e} \\sum_{e=1}^{N_e} \\left( r^{(e)}(t) + \\sum_i \\gamma^i r^{(e)}(t+i) \\right), $$\n",
    "where $r^{(e)}$ is the reward obtained during episode number $e$, and $N_e$ is the number of episodes collected.\n",
    "\n",
    "Compute the expected return of policy *plcs.policy_epsilon1()* from the trajectories you collected, compare it with the return of the deterministic policy and comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totG = 0.0\n",
    "for i in range(n_episodes):\n",
    "    None\n",
    "\n",
    "hatG = totG / n_episodes   \n",
    "print('Return: {0}'.format(hatG))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: enter your comment here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy improvement\n",
    "\n",
    "So far, we have only considered the problem of evaluating how good a given policy is. We now consider a second central problem in reinforcement learning, that is, **policy improvement**: *given a policy, how can we construct a better policy?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State values and Action-state values\n",
    "In the previous section we have evaluated policies by computing or estimating their overall return. This is a **global** value that assesses the goodness of a predefined policy from the beginning to the end. Comparing two policies according to their overall return is simple. But improving policies relying only on this quantity is tricky. Generating new policies, computing their overall return, and comparing with existing policies is unfeasible, as the number of possible policies explodes with the number of states and actions.\n",
    "\n",
    "Ideally, we need some form of **local** information that would allow us to assemble new policies in a *modular* way. These quantities are:\n",
    "- **state values** $v(s)$: evaluating how good is it to be in a certain state $s$ (more precisely, what is the expected return for a certain state $s$).\n",
    "- **action-state values** $q(s,a)$: evaluating how good is it to take a certain action $a$ being in a certain state $s$ (more precisely, what is the expected return from a certain state $s$ by taking action $a$).\n",
    "By knowing locally how good is a certain state or how good is a certain action we can steer the choices towards better policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State values\n",
    "\n",
    "The state value $v(s)$ of a state $s$ under the policy $\\pi$ is defined as:\n",
    "$$\n",
    "v_{\\pi}(s) = \\mathbb{E} \\left[ G(t) \\vert S(t)=s \\right],\n",
    "$$\n",
    "that is the return, as we computed above, but starting now from state $s$ instead of the initial state.\n",
    "\n",
    "As before, computing the return for a stochastic policy is not immediate, and it requires relying on one of the approaches referenced above (*dynamic programming, Monte Carlo, temporal differences*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Policy Evaluation for State Values\n",
    "\n",
    "The same Monte Carlo principle we used to estimate overall returns may be used to compute state values: we run several episodes of the agents acting in the world and then we compute the state values.\n",
    "\n",
    "First we generate the data. We use the same environment, but we now consider as a starting policy a completely random policy. This choice represents our complete ignorance about the environment (no one provided us with an almost optimal policy) and it will allow the agent to explore the environment more widely.\n",
    "\n",
    "Load *wlds.world1()* and *plcs.policy_random()*, and run 100 episodes with this setting while collecting the trajectories of your simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 100\n",
    "episodes = []\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to compute the state value function $v(s)$ of each state $s$ using the *first-visit* Monte Carlo algorithm. For each state $s$, we find the first instance in an episode and compute its return. At the end we compute the expected return.\n",
    "\n",
    "Compute the state value function using the first-visit Monte Carlo algorithm. \n",
    "1. Iterate over every state; a state is encoded as *gw.Loc()* object from the module *gw*; the constructor *gw.Loc(x,y)* receives the $x,y$ coordinates of a square in the grid world.\n",
    "2. Iterate over the episodes; in each episode find the first occurence of the current state; you can use the function *gw.find_location_in_array()* from the module *gw*. The function *gw.find_location_in_array(loc, list)* receives a location *loc* in format *gw.Loc()* and a list of locations; it returns the index of the first instance of *loc* within *list*, or *None* if *loc* does not appear in *list*.\n",
    "3. Compute the return from the first occurrence of the current state.\n",
    "4. Average the return over all the episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vs = {}\n",
    "\n",
    "for x in range(4):\n",
    "    for y in range (4):\n",
    "        s = gw.Loc(x,y)\n",
    "        \n",
    "        None\n",
    "            \n",
    "        Vs[s] = np.mean(Gs)          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now place the state-values in a matrix V (respecting the convention discussed at the beginning on the labelling of the squares), print the matrix, and comment on the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = np.zeros((4,4))\n",
    "for x in range(4):\n",
    "    for y in range (4):\n",
    "        None\n",
    "        \n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: enter your comment here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Improvement\n",
    "Our agent started with a purely random policy. Now that we have computed the state values of our grid world we can select an optimal policy. A simple way to use the state-value matrix to improve on our random policy is the following one: at each state select the action that leads you to the successor state with highest value.\n",
    "\n",
    "Define a policy (a function receiving a state location as an input) using the state-value matrix you computed above to define an improved policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_policy(state):\n",
    "    value_next_state = {}\n",
    "    \n",
    "    if(state.x>0):\n",
    "        value_next_state[const.UP] = V[state.x-1,state.y]\n",
    "    else:\n",
    "        value_next_state[const.UP] = -np.inf\n",
    "        \n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sort of policy is called a *greedy policy* because, at each step, it always deterministically takes the action that is expected to produce the higher return.\n",
    "\n",
    "Load *wlds.world1()* and use the greedy policy you have just defined to control the agent. Plot the outcome, and comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = wlds.world1()\n",
    "\n",
    "state, reward, termination, msgs = W.reset()\n",
    "while(None):\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: enter your comment here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A completely deterministic policy as the one we defined is always susceptible to underperform if the environment undergoes any change.\n",
    "\n",
    "Design a world, possibly with minimal changes from *wlds.world1()*, where the optimal policy just learned fails. Run this world for 10 iterations using the learned greedy policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = gw.GridWorld(4,4, 0,0, 3,3)\n",
    "W.add_wall(None)\n",
    "\n",
    "state, reward, termination, msgs = W.reset()\n",
    "for _ in range(None):\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, while learning and acting in the environment, it is a good idea to preserve some degree of *stochasticity* to keep analyzing the world and improve your policy.\n",
    "\n",
    "Improving the policy of an agent is a central problem in reinforcement learning. The problem of **control** consists in finding or approximating an optimal policy. A common approach, which we have followed in this notebook, consists in alternating a step of *policy evaluation* and a step of *policy improvement*. In the example above, we have started with a random policy and we have computed the associated state-value matrix (policy evaluation); then, we have defined a greedy policy using the state-value matrix (policy improvement). This generic learning approach is called **Generalized Policy Iteration**. On more complex problem, several iterations of evaluation-improvement may be required to approximate an optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: Q-Learning\n",
    "\n",
    "**Q-Learning** is an *off-policy*, *temporal difference* reinforcement learning algorithm. \n",
    "Off-policy means that the agent learns an optimal target policy $\\pi_t$ while behaving according to a different behaviour policy $\\pi_b$; this allows, for instance, the agent to behave according to a sub-optimal exploratory policy while learning and memorizing an optimal policy. Temporal difference means that the agent uses an algorithm that allows learning in real-time without the need to collect whole episodes (as it was in the case of Monte Carlo).\n",
    "\n",
    "Q-Learning does not rely on any model of the environment, and as such, it works estimating *state-action values* $q(s,a)$ instead of *state values* $v(s)$.\n",
    "\n",
    "First, load *wlds.world1()* and randomly initialize a matrix/tensor to store the state-action values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = wlds.world1()\n",
    "\n",
    "Q = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While acting in the world, we are going to define as a *behaviour policy* an $\\epsilon$-greedy policy that, for each state, takes a random action with probability $\\epsilon$ or the optimal action defined by the state-action matrix $q(s,a)$ with probability $1-\\epsilon$.\n",
    "\n",
    "Implement this policy as a function receiving as input the current state in the form of *gw.Loc()*, the current state-action matrix $q(s,a)$, and the parameter $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def behaviour_policy(state,Q,eps):\n",
    "    \n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start our learning with the following parameters: discount factor $\\gamma = 0.9$, learning rate $\\alpha=0.1$, exploration stochasticity $\\epsilon = 0.4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "epsilon = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement and run Q-learning for one episode.\n",
    "\n",
    "1. Given that we are in state $s$, we choose an action $a$ according to our behaviour policy *behaviour_policy()*\n",
    "2. We take action $a$, receive reward $r$ and end up in state $s'$\n",
    "3. We update the action-value function $q(s,a)$ according to the following formula:\n",
    "$$\n",
    "q(s,a) \\leftarrow q(s,a) + \\alpha \\left[ r + \\gamma \\max_{a'} q(s',a') - q(s,a) \\right]\n",
    "$$\n",
    "where $\\max_{a'} q(s',a')$ is the action-value of the action $a'$ that maximizes the action-value for state $s'$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, termination, msgs = W.reset()\n",
    "while(None):\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the values in the action-value matrix, for instance in the pre-final states (2,3) or (3,2). Comment on the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: enter your comment here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the *mean* and *max* operator over the actions to compute an average and a max version of a state-value matrix from the action-state matrix (i.e.: average/max over the action). Comment on the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: enter your comment here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Q-learning for $10$ more episodes (without resetting your action-value matrix!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 10\n",
    "\n",
    "for _ in range(n_episodes):\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print again from the action-state matrix, and comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: enter your comment here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider how learning would change if you were to change the learning parameter, or if the environment were to change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: enter your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Approximators\n",
    "So far we have solved the reinforcement learning problem using tables and matrices to store state-value, $v$, functions and action-value, $q$, functions. This is called the **tabular** approach to RL. This solution is simple and interpretable, but it has a severe limitation.\n",
    "\n",
    "What happens if our gridworld becomes much bigger, extending to a 10000 by 10000 grid? How much memory would we need to store the state-value or the action-value function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: enter your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you imagine a way to solve this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: enter your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In this notebook we studied the basics of **reinforcement learning** through the analysis on a simple *gridworld* environment of two core RL problems: **policy evaluation** and **policy improvement**.\n",
    "\n",
    "We first considered the problem of policy evaluation, that is, the problem of assessing how good a policy is. For **deterministic policies** we solved the problem simply by computing the *total discounted return* of a policy. After realizing the limitation of deterministic policies, we turned to **stochastic policies**, and we computed their *return in expectation* relying on *Monte Carlo methods*.\n",
    "\n",
    "Next we considered the problem of policy improvement, that is, the problem of learning optimal policies. For this task, we realized that a global value for a policy is not handy when improving a policy, and, instead local values would be more useful. We then computed **state values** and **action values** to assess a policy step by step, and used this values to take improved actions. This iterative approach, based on evaluating a policy and improving the policy, is an instance of the more generic method called **Generalized Policy Iteration**.\n",
    "\n",
    "In the final optional part, we considered a more advanced RL algorithm, **Q-learning**. Q-learning allowed us to learn an optimal policy while interacting with an environment; this algorithm had two important features: (i) it learns in a continuous way, step-by-step, and not on complete episodes; (ii) it learns an optimal policy while behaving according to a different, more exploration-prone, policy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
